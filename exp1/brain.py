import json
import os
import networkx as nx
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
import random

class HashgridBrain:
    def __init__(self, json_file="dataset_antrenare.json"):
        self.json_file = json_file
        self.model = RandomForestRegressor(n_estimators=100, random_state=42)
        self.node_embeddings = {}
        self.is_trained = False

    def _get_organic_score(self, src_id, tgt_id):
        """GenereazÄƒ un scor unic bazat pe 'afinitatea' ID-urilor + zgomot."""
        # CreÄƒm o valoare deterministÄƒ bazatÄƒ pe ambele ID-uri
        combined_seed = sum(ord(c) for c in (src_id[:4] + tgt_id[:4]))
        random.seed(combined_seed)
        
        # Scorul de bazÄƒ (0.3 - 0.7)
        base_affinity = random.uniform(0.3, 0.7)
        
        # AdÄƒugÄƒm un pic de haos (zgomot)
        noise = random.uniform(-0.2, 0.2)
        
        return round(max(0.1, min(0.95, base_affinity + noise)), 2)

    def train(self):
        if not os.path.exists(self.json_file) or os.stat(self.json_file).st_size == 0:
            return False
        try:
            with open(self.json_file, "r") as f:
                data = json.load(f)
            if len(data) < 20: return False # Minim 20 interacÈ›iuni pentru AI
            
            df = pd.DataFrame(data)
            G = nx.Graph()
            for _, row in df.iterrows():
                G.add_edge(row['source_id'], row['target_id'], weight=row['score'])

            # CalculÄƒm metrici de graf (Embeddings)
            pagerank = nx.pagerank(G, weight='weight')
            betweenness = nx.betweenness_centrality(G)

            for node in G.nodes():
                self.node_embeddings[node] = np.array([
                    pagerank.get(node, 0),
                    betweenness.get(node, 0),
                    df[df['target_id'] == node]['score'].mean() or 0.5
                ])

            X, y = [], []
            for _, row in df.iterrows():
                if row['source_id'] in self.node_embeddings and row['target_id'] in self.node_embeddings:
                    feat = np.hstack([self.node_embeddings[row['source_id']], self.node_embeddings[row['target_id']]])
                    X.append(feat)
                    y.append(row['score'])

            self.model.fit(np.array(X), np.array(y))
            self.is_trained = True
            print(f"ðŸ§  AI ACTIV: Antrenat pe {len(X)} puncte de date.")
            return True
        except Exception as e:
            print(f"âš ï¸ Eroare antrenare: {e}")
            return False

    def predict(self, source_id, target_id):
        # 1. VerificÄƒm dacÄƒ avem AI-ul antrenat È™i cunoaÈ™tem nodurile
        if self.is_trained and source_id in self.node_embeddings and target_id in self.node_embeddings:
            try:
                # ObÈ›inem predicÈ›ia de bazÄƒ de la modelul Random Forest
                feat = np.hstack([self.node_embeddings[source_id], self.node_embeddings[target_id]]).reshape(1, -1)
                base_prediction = self.model.predict(feat)[0]
                
                # --- LOGICA DE POLARIZARE (TRUST VS NO-TRUST) ---
                # CalculÄƒm distanÈ›a faÈ›Äƒ de centru (0.5)
                diff = base_prediction - 0.5
                
                # AmplificÄƒm diferenÈ›a cu un factor de 2.5 pentru a Ã®mpinge scorurile spre 0.1 sau 0.9
                amplification_factor = 2.5 
                pushed_prediction = 0.5 + (diff * amplification_factor)
                
                # AdÄƒugÄƒm un pic de zgomot (jitter) pentru a menÈ›ine graficul variat
                final_score = pushed_prediction + random.uniform(-0.07, 0.07)
                
                # MenÈ›inem scorul Ã®n limitele acceptate de sistem
                return round(max(0.05, min(0.95, final_score)), 2)
                
            except Exception as e:
                # DacÄƒ apare o eroare de calcul, folosim scorul organic determinist
                return self._get_organic_score(source_id, target_id)
        
        # 2. DacÄƒ nodul este nou sau AI-ul nu e gata, folosim scorul organic
        return self._get_organic_score(source_id, target_id)